{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VvoiWaQgka87",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.60744859826E12,
     "user_tz": -330.0,
     "elapsed": 35933.0,
     "user": {
      "displayName": "ruzbeh irani",
      "photoUrl": "",
      "userId": "08825081548842265193"
     }
    },
    "outputId": "5d471390-2dd2-448e-854a-58d1d7fb2927"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "IRrURav8zX8u",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.607463433922E12,
     "user_tz": -330.0,
     "elapsed": 1437.0,
     "user": {
      "displayName": "ruzbeh irani",
      "photoUrl": "",
      "userId": "08825081548842265193"
     }
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "y27gA__B9h89",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.60746343472E12,
     "user_tz": -330.0,
     "elapsed": 2217.0,
     "user": {
      "displayName": "ruzbeh irani",
      "photoUrl": "",
      "userId": "08825081548842265193"
     }
    },
    "outputId": "4242b507-7927-4461-beb8-25af6a8f190d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Colab Notebooks/DLFinalProject\n"
     ]
    }
   ],
   "source": [
    "cd /content/drive/MyDrive/Colab Notebooks/DLFinalProject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_IbX7ZMpiZJC",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.607463434721E12,
     "user_tz": -330.0,
     "elapsed": 2147.0,
     "user": {
      "displayName": "ruzbeh irani",
      "photoUrl": "",
      "userId": "08825081548842265193"
     }
    },
    "outputId": "65dcc658-eb43-47a3-ee6b-21feafa660d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec  8 21:37:14 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   47C    P0    56W / 300W |   3295MiB / 16130MiB |      0%      Default |\n",
      "|                               |                      |                 ERR! |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KFn3tR0-igWQ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.607463434722E12,
     "user_tz": -330.0,
     "elapsed": 1970.0,
     "user": {
      "displayName": "ruzbeh irani",
      "photoUrl": "",
      "userId": "08825081548842265193"
     }
    },
    "outputId": "1d6f1a5a-85e7-4389-c1c3-e4d79b120aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "le1wxa1MktEk"
   },
   "source": [
    "This is for Installing all requirements needed for our experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ClLKyH_q50v",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.607463439917E12,
     "user_tz": -330.0,
     "elapsed": 7054.0,
     "user": {
      "displayName": "ruzbeh irani",
      "photoUrl": "",
      "userId": "08825081548842265193"
     }
    },
    "outputId": "e5d0568a-b276-4d56-ea75-5b0c0cabcb09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: adapter-transformers in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from adapter-transformers) (3.0.12)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from adapter-transformers) (1.18.5)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from adapter-transformers) (3.12.4)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from adapter-transformers) (0.8)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from adapter-transformers) (4.41.1)\n",
      "Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from adapter-transformers) (0.9.3)\n",
      "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from adapter-transformers) (0.1.91)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from adapter-transformers) (2.23.0)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from adapter-transformers) (0.0.43)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from adapter-transformers) (20.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from adapter-transformers) (2019.12.20)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->adapter-transformers) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->adapter-transformers) (50.3.2)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->adapter-transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->adapter-transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->adapter-transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->adapter-transformers) (2020.11.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->adapter-transformers) (0.17.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->adapter-transformers) (7.1.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->adapter-transformers) (2.4.7)\n"
     ]
    }
   ],
   "source": [
    "#!pip install -q --force-reinstall transformers\n",
    "# !pip install -q --force-reinstall git+https://github.com/adapter-hub/adapter-transformers.git\n",
    "!pip install adapter-transformers\n",
    "# install datasets\n",
    "!pip install -q datasets\n",
    "\n",
    "# Make sure that we have a recent version of pyarrow in the session before we continue - otherwise reboot Colab to activate it\n",
    "import pyarrow\n",
    "if int(pyarrow.__version__.split('.')[1]) < 16 and int(pyarrow.__version__.split('.')[0]) == 0:\n",
    "    import os\n",
    "    os.kill(os.getpid(), 9)\n",
    "\n",
    "#!git clone https://github.com/huggingface/transformers\n",
    "#!python transformers/utils/download_glue_data.py --tasks SST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VNRBCGgkx7d"
   },
   "source": [
    "This code block get all data from allennlp this is a one time activity once data is downloaded same files can be used for each runs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7EdP18g0irGz",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.607463439919E12,
     "user_tz": -330.0,
     "elapsed": 6878.0,
     "user": {
      "displayName": "ruzbeh irani",
      "photoUrl": "",
      "userId": "08825081548842265193"
     }
    }
   },
   "outputs": [],
   "source": [
    "# !curl --create-dirs -Lo ./data/citation_intent/train.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/citation_intent/train.jsonl\n",
    "# !curl --create-dirs -Lo ./data/citation_intent/dev.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/citation_intent/dev.jsonl\n",
    "# !curl --create-dirs -Lo ./data/citation_intent/test.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/citation_intent/test.jsonl\n",
    "\n",
    "# !curl --create-dirs -Lo ./data/sciie/train.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/sciie/train.jsonl\n",
    "# !curl --create-dirs -Lo ./data/sciie/dev.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/sciie/dev.jsonl\n",
    "# !curl --create-dirs -Lo ./data/sciie/test.jsonl https://allennlp.s3-us-west-2.amazonaws.com/dont_stop_pretraining/data/sciie/test.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "1Wt8_gPVi-3Z",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.607463440577E12,
     "user_tz": -330.0,
     "elapsed": 7367.0,
     "user": {
      "displayName": "ruzbeh irani",
      "photoUrl": "",
      "userId": "08825081548842265193"
     }
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "import dataclasses\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Optional\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from transformers import AutoTokenizer, EvalPrediction, AutoModelWithHeads, AdapterType\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    glue_compute_metrics,\n",
    "    glue_tasks_num_labels,\n",
    "    set_seed,\n",
    "    AdapterType,CONFIG_MAPPING,\n",
    "    MODEL_WITH_LM_HEAD_MAPPING,\n",
    "    AdapterArguments,\n",
    "    AdapterConfig,\n",
    "    AdapterType,\n",
    "    AutoConfig,\n",
    "    AutoModelWithLMHead,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    HfArgumentParser,\n",
    "    LineByLineTextDataset,\n",
    "    PreTrainedTokenizer,\n",
    "    TextDataset,\n",
    "    Trainer,\n",
    "    TrainingArguments, \n",
    "    AutoModelForMaskedLM, \n",
    "    PfeifferConfig, \n",
    "    HoulsbyConfig, \n",
    "    InvertibleAdapterConfig\n",
    ")\n",
    "\n",
    "from typing import Any, Iterable, List, NewType, Optional, Tuple, Union\n",
    "import dataclasses\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "model_name = \"roberta-base\"\n",
    "label_count = {'citation_intent':6, 'sciie':7}\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, max_len=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0nA6xZIlCKG"
   },
   "source": [
    "**Task Adapter Config**\n",
    "This is used for modify task adapter config for our ablation study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "IdJT4CfywIag",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.607463440579E12,
     "user_tz": -330.0,
     "elapsed": 1431.0,
     "user": {
      "displayName": "ruzbeh irani",
      "photoUrl": "",
      "userId": "08825081548842265193"
     }
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CustomConfig(AdapterConfig):\n",
    "    \"\"\"\n",
    "    The adapter architecture proposed by Pfeiffer et. al., 2020.\n",
    "    Described in https://arxiv.org/pdf/2005.00247.pdf.\n",
    "    \"\"\"\n",
    "\n",
    "    original_ln_before: bool = True\n",
    "    original_ln_after: bool = True\n",
    "    residual_before_ln: bool = True\n",
    "    adapter_residual_before_ln: bool = False\n",
    "    ln_before: bool = False\n",
    "    ln_after: bool = False\n",
    "    mh_adapter: bool = True\n",
    "    output_adapter: bool = False\n",
    "    non_linearity: str = \"relu\"\n",
    "    reduction_factor: int = 16\n",
    "    invertible_adapter: Optional[dict] = InvertibleAdapterConfig(\n",
    "        block_type=\"nice\", non_linearity=\"relu\", reduction_factor=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STQKip_ClSsT"
   },
   "source": [
    "**Trainer Setup**\n",
    "\n",
    "\n",
    "This block stiches everything and readies a trainer for our execution\n",
    "\n",
    "1.   Parses file and creates a dataset\n",
    "2.   Fetches pretrained model\n",
    "3.   if language adapter to be used then Attaches language Adapter \n",
    "4.   Load language adapter from file system which was trained in previous notebook\n",
    "5.   Attaches a task adapter\n",
    "6.   Creates a trainer with all datasets and training arguments declared as constants in the code\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "w4xZfTRCjL0T",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1.607463441016E12,
     "user_tz": -330.0,
     "elapsed": 1176.0,
     "user": {
      "displayName": "ruzbeh irani",
      "photoUrl": "",
      "userId": "08825081548842265193"
     }
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx], device=device) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], device=device)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def getDF(filename):\n",
    "    with open(filename) as f:\n",
    "        content = f.readlines()\n",
    "    data = []\n",
    "    for t in content:\n",
    "      data.append(json.loads(t))\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "def getDataset(dataset='citation_intent'):\n",
    "    le = LabelEncoder()\n",
    "\n",
    "    filename = f\"data/{dataset}/train.jsonl\"\n",
    "    new_df = getDF(filename)\n",
    "    new_df['ENCODE_CAT'] = le.fit_transform(new_df['label'])\n",
    "\n",
    "    filename = f\"data/{dataset}/dev.jsonl\"\n",
    "    dev_df = getDF(filename)\n",
    "    dev_df['ENCODE_CAT'] = le.fit_transform(dev_df['label'])\n",
    "\n",
    "    filename = f\"data/{dataset}/test.jsonl\"\n",
    "    test_df = getDF(filename)\n",
    "    test_df['ENCODE_CAT'] = le.fit_transform(test_df['label'])\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    train_df =new_df.reset_index(drop=True)\n",
    "    dev_df =new_df.reset_index(drop=True)\n",
    "    test_df =test_df.reset_index(drop=True)\n",
    "\n",
    "    train_encodings = tokenizer(train_df['text'].tolist() , truncation=True, padding=True)\n",
    "    dev_encodings = tokenizer(dev_df['text'].tolist() , truncation=True, padding=True)\n",
    "    test_encodings = tokenizer(test_df['text'].tolist(), truncation=True, padding=True)\n",
    "\n",
    "    train_dataset = CustomDataset(train_encodings, train_df['ENCODE_CAT'].tolist())\n",
    "    dev_dataset = CustomDataset(dev_encodings, dev_df['ENCODE_CAT'].tolist())\n",
    "    test_dataset = CustomDataset(test_encodings, test_df['ENCODE_CAT'].tolist())\n",
    "\n",
    "    return train_dataset, dev_dataset, test_dataset\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "def trainModelwithClassificationHead(model_name, dataset_name=\"citation_intent\", seed=1, adapterName='pfieffer'):\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "    logging_steps=10000, \n",
    "    per_device_train_batch_size=4, \n",
    "    per_device_eval_batch_size=4, \n",
    "    save_steps=20000,\n",
    "    evaluate_during_training=True,\n",
    "    output_dir=f\"./models/{dataset_name}_{adapterName}\",\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    do_predict=True,\n",
    "    learning_rate=0.0001,\n",
    "    num_train_epochs=20,\n",
    "    seed = seed,\n",
    "    save_total_limit = 1,\n",
    "    gradient_accumulation_steps = 1\n",
    "    )\n",
    "\n",
    "    train_dataset, dev_dataset, test_dataset = getDataset(dataset=dataset_name)\n",
    "\n",
    "    if adapterName == 'pfieffer':\n",
    "\n",
    "        language = 'tapt-pfieffer'\n",
    "        adapter_dir = '/content/drive/MyDrive/Colab Notebooks/DLFinalProject/TAPT-PF'\n",
    "        model = AutoModelWithHeads.from_pretrained(model_name)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        adapter_config = PfeifferConfig()\n",
    "        model.add_adapter(language, AdapterType.text_lang, config=adapter_config)\n",
    "        model.from_pretrained(adapter_dir, local_files_only=True)\n",
    "\n",
    "    elif adapterName == 'houlsby':\n",
    "\n",
    "        language = 'tapt-houlsby'\n",
    "        adapter_dir = '/content/drive/MyDrive/Colab Notebooks/DLFinalProject/TAPT_houlsby/TAPT_houlsby'\n",
    "        model = AutoModelWithHeads.from_pretrained(model_name)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        adapter_config = HoulsbyConfig(invertible_adapter=InvertibleAdapterConfig(block_type='nice', non_linearity='relu', reduction_factor=2))\n",
    "        model.add_adapter(language, AdapterType.text_lang, config=adapter_config)\n",
    "        model.from_pretrained(adapter_dir, local_files_only=True)\n",
    "    \n",
    "    elif adapterName == 'no_lang_adapter':\n",
    "\n",
    "        model = AutoModelWithHeads.from_pretrained(model_name)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    taskAdapterConfig = CustomConfig()\n",
    "    model.add_adapter(dataset_name, AdapterType.text_task, config=taskAdapterConfig)\n",
    "    model.add_classification_head(dataset_name, num_labels=label_count[dataset_name], overwrite_ok=True)\n",
    "    \n",
    "    if adapterName != 'no_lang_adapter':\n",
    "        model.set_active_adapters([language, dataset_name])\n",
    "    else:\n",
    "        model.set_active_adapters([dataset_name])\n",
    "    \n",
    "    model.train_adapter([dataset_name])\n",
    "    \n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=dev_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    print(trainer.args.device)\n",
    "    trainer.train()\n",
    "    trainer.evaluate()\n",
    "\n",
    "    # get test results\n",
    "    _, _, metrics = trainer.predict(test_dataset)\n",
    "    with open(f'/content/drive/MyDrive/Colab Notebooks/DLFinalProject/results/results_allTFT2.txt', mode='a') as f:\n",
    "        f.write(f\"Test results: \\n {adapterName} \\n {dataset_name} \\n {metrics} \\n\\n\")\n",
    "    f.close()\n",
    "\n",
    "    return metrics['eval_f1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wYfHd4TlXGI"
   },
   "source": [
    "**Trainer Execution**\n",
    "\n",
    "\n",
    "This block is our final execution block this will run our code for each defined test test for 5 rounds each and will store the result s into a file with its mean F1 score and standard deviation of our F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gaGzrc6QnP_8",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282.0
    },
    "outputId": "aa169898-a58a-41e8-f4c4-2c94aea0982c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at /content/drive/MyDrive/Colab Notebooks/DLFinalProject/TAPT_houlsby/TAPT_houlsby were not used when initializing RobertaModelWithHeads: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaModelWithHeads from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModelWithHeads from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModelWithHeads were not initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/DLFinalProject/TAPT_houlsby/TAPT_houlsby and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='5452' max='16100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5452/16100 05:11 < 10:08, 17.50 it/s, Epoch 6.77/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results with adapters\n",
    "import itertools\n",
    "nRounds = 3\n",
    "dataset_names = ['citation_intent', 'sciie']\n",
    "adapter_names = ['pfieffer', 'houlsby', 'no_lang_adapter']\n",
    "exclude_experiments = [('citation_intent','pfieffer'),('citation_intent','houlsby'),('citation_intent','no_lang_adapter'),('sciie','pfieffer')]\n",
    "for dn, an in itertools.product(dataset_names, adapter_names):\n",
    "    if (dn, an) in exclude_experiments:\n",
    "        continue\n",
    "    f1 = []\n",
    "    for i in range(nRounds):\n",
    "        f1.append(trainModelwithClassificationHead(model_name, \n",
    "                                                   dataset_name=dn, \n",
    "                                                   seed=i, \n",
    "                                                   adapterName=an))\n",
    "    average = np.mean(f1)\n",
    "    std = np.std(f1)\n",
    "    var = np.var(f1)\n",
    "    with open(f'/content/drive/MyDrive/Colab Notebooks/DLFinalProject/results/results_summaryTFT2.txt', mode='a') as f:\n",
    "        f.write(f\"Test results: \\n {an} \\n {dn} \\n Average Macro F1:{average} \\n StdDev:{std} \\n Variance:{var}\\n\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7cvo9oVhJ7Py"
   },
   "outputs": [],
   "source": [
    "cat results_summaryTFT2.txt"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "TaskAdapterNotebook",
   "provenance": [
    {
     "file_id": "1QkWsJs15WhEOmMM1dMF5LxEZ-DAubdcG",
     "timestamp": 1.607427529102E12
    },
    {
     "file_id": "1l6QTlqapwVY0WNDEtq9rheQWfH_7nQVh",
     "timestamp": 1.607188282164E12
    },
    {
     "file_id": "https://gist.github.com/ruzbeh/4b790c5d31016c700ce7f49c519bcb71#file-adapter_v2-ipynb",
     "timestamp": 1.607187488783E12
    }
   ],
   "collapsed_sections": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
